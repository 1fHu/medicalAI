{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63bcb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedfe744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetNetClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim=45,          # 每个细胞的特征维度\n",
    "                 hidden_dim=128,        # 细胞特征编码后的维度\n",
    "                 classifier_hidden=128, # MLP中间层\n",
    "                 output_dim=3):         # 类别数（分类）\n",
    "        super().__init__()\n",
    "\n",
    "        # Set Encoder: 对每个细胞特征进行变换\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # 分类器 MLP（输入维度是 2×hidden_dim，因为拼接了 mean 和 sum）\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, classifier_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(classifier_hidden, output_dim)\n",
    "            # add softmax\n",
    "        )\n",
    "\n",
    "    def forward(self, batch_cells):\n",
    "        \"\"\"\n",
    "        batch_cells: list of tensors, each of shape [N_i, 45]\n",
    "        return: logits tensor of shape [B, 3]\n",
    "        \"\"\"\n",
    "        logits_list = []\n",
    "\n",
    "        for cells in batch_cells:  # 每一组细胞\n",
    "            z = self.encoder(cells)               # [N, hidden_dim]\n",
    "            z_mean = z.mean(dim=0)                # [hidden_dim]\n",
    "            z_sum = z.sum(dim=0)                  # [hidden_dim]\n",
    "            group_embedding = torch.cat([z_mean, z_sum], dim=0)  # [2 * hidden_dim]\n",
    "            logits = self.classifier(group_embedding)            # [3]\n",
    "            logits_list.append(logits)\n",
    "\n",
    "        return torch.stack(logits_list)  # [B, 3]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66f202dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellGroupDataset(Dataset):\n",
    "    def __init__(self, npy_path, label_list):\n",
    "        self.raw_data = np.load(npy_path, allow_pickle=True)  # shape: [40]\n",
    "        self.labels = label_list  # shape: [40]，例如 [0, 2, 1, ...]\n",
    "        \n",
    "        # 每组为 list of np.array → 将每个细胞拼接为 torch tensor\n",
    "        self.data = []\n",
    "        for group in self.raw_data:\n",
    "            group_tensor = []\n",
    "            for cell in group:\n",
    "                cell_feat = np.concatenate([cell[0].flatten(), cell[1].flatten()])  # shape (45,)\n",
    "                cell_feat_tensor = torch.tensor(cell_feat, dtype=torch.float32)\n",
    "                group_tensor.append(cell_feat_tensor)\n",
    "            self.data.append(group_tensor)  # list of tensors per group\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# collate_fn 处理变长 batch\n",
    "def collate_fn(batch):\n",
    "    data, labels = zip(*batch)\n",
    "    return list(data), torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e685d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你自己的标签（例如 0 → class 0, 0.5 → class 1, 1 → class 2）\n",
    "float_labels = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0.5, \n",
    "                0.5, 0.5, 0.5, 0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0.5, \n",
    "                0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]  # 长度 = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3232c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_map = {0.0: 0, 0.5: 1, 1.0: 2}\n",
    "class_labels = [label_map[val] for val in float_labels]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 Dataset\n",
    "dataset = CellGroupDataset(\"./dataset_first.npy\", class_labels)\n",
    "\n",
    "# 负责打包\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7da9ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss = 115952.2812\n",
      "Logits sample: [ -54836.598  129788.17  -438236.97 ]\n",
      "Labels sample: 2\n",
      "Epoch 2, Loss = 1121383.0000\n",
      "Logits sample: [-76073.734  30626.912 -92088.1  ]\n",
      "Labels sample: 0\n",
      "Epoch 3, Loss = 187645.0156\n",
      "Logits sample: [-262130.19   -18247.143 -115872.586]\n",
      "Labels sample: 1\n",
      "Epoch 4, Loss = 51118.2578\n",
      "Logits sample: [-3231345.8   -942575.8   -240301.78]\n",
      "Labels sample: 2\n",
      "Epoch 5, Loss = 93108.2422\n",
      "Logits sample: [-2814.9766   814.7249 -1550.7041]\n",
      "Labels sample: 1\n",
      "Epoch 6, Loss = 68588.9375\n",
      "Logits sample: [-591837.44   -60465.223  -81041.54 ]\n",
      "Labels sample: 1\n",
      "Epoch 7, Loss = 110411.7109\n",
      "Logits sample: [-113985.44    -17859.018    -2997.1104]\n",
      "Labels sample: 0\n",
      "Epoch 8, Loss = 25992.3809\n",
      "Logits sample: [-115119.664   -15443.127     2423.8503]\n",
      "Labels sample: 0\n",
      "Epoch 9, Loss = 201737.8125\n",
      "Logits sample: [-60174.33    -5489.643   -7294.5796]\n",
      "Labels sample: 0\n",
      "Epoch 10, Loss = 182416.4062\n",
      "Logits sample: [-349654.88   -95280.45    28354.459]\n",
      "Labels sample: 0\n"
     ]
    }
   ],
   "source": [
    "model = SetNetClassifier(input_dim=45, hidden_dim=128, output_dim=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 可选 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for batch_cells, batch_labels in loader:\n",
    "        batch_cells = [torch.stack([cell.to(device) for cell in group]) for group in batch_cells]\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        logits = model(batch_cells)  # [B, 3]\n",
    "        loss = criterion(logits, batch_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss = {loss.item():.4f}\")\n",
    "    print(f\"Logits sample: {logits[0].detach().cpu().numpy()}\")\n",
    "    print(f\"Labels sample: {batch_labels[0].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d062c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
